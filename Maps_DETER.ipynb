{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93bc616-d1f2-4b7a-be4d-e38c07f92ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "import re\n",
    "from folium.plugins import MarkerCluster\n",
    "import os\n",
    "from folium.plugins import TimestampedGeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c9357c-3cf6-40ea-839b-0afc9a70e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # DETER alerts_df_df_df (GEODF)\n",
    "    alerts_df = gpd.read_file('../data/deter-amz-public-2024out08/deter-amz-deter-public.shp', encoding='utf-8')\n",
    "    alerts_df.loc[alerts_df['CLASSNAME'] == 'DEGRDACAO', 'CLASSNAME'] = 'DEGRADACAO'\n",
    "    alerts_df = alerts_df[~(alerts_df['CLASSNAME'] == 'CORTE_SELETIVO')]\n",
    "    alerts_df['VIEW_DATE'] = pd.to_datetime(alerts_df['VIEW_DATE'])\n",
    "    alerts_df['ANO'] = alerts_df['VIEW_DATE'].dt.year\n",
    "    alerts_df['MES'] = alerts_df['VIEW_DATE'].dt.month\n",
    "    alerts_df['MES/ANO'] = alerts_df['VIEW_DATE'].dt.strftime('%Y-%m')\n",
    "\n",
    "    # DETER ALERTS (CSV)\n",
    "    df_deter = pd.DataFrame(alerts_df)\n",
    "    df_deter = df_deter.drop(columns=['FID', 'QUADRANT', 'PATH_ROW', 'SENSOR', 'SATELLITE', 'geometry'])\n",
    "\n",
    "    \n",
    "    #IBGE DATA\n",
    "    legal_amazon = gpd.read_file('../data/brazilian_legal_amazon/brazilian_legal_amazon.shp',encoding='utf-8')\n",
    "    states = gpd.read_file('../data/states_legal_amazon/states_legal_amazon.shp',encoding='utf-8')\n",
    "    \n",
    "    ac = gpd.read_file('../data/malhas_regionais_ibge/AC_Municipios_2022/AC_Municipios_2022.shp', encoding='utf-8')\n",
    "    am = gpd.read_file('../data/malhas_regionais_ibge/AM_Municipios_2022/AM_Municipios_2022.shp', encoding='utf-8')\n",
    "    ap = gpd.read_file('../data/malhas_regionais_ibge/AP_Municipios_2022/AP_Municipios_2022.shp', encoding='utf-8')\n",
    "    ma = gpd.read_file('../data/malhas_regionais_ibge/MA_Municipios_2022/MA_Municipios_2022.shp', encoding='utf-8')\n",
    "    mt = gpd.read_file('../data/malhas_regionais_ibge/MT_Municipios_2022/MT_Municipios_2022.shp', encoding='utf-8')\n",
    "    pa = gpd.read_file('../data/malhas_regionais_ibge/PA_Municipios_2022/PA_Municipios_2022.shp', encoding='utf-8')\n",
    "    ro = gpd.read_file('../data/malhas_regionais_ibge/RO_Municipios_2022/RO_Municipios_2022.shp', encoding='utf-8')\n",
    "    rr = gpd.read_file('../data/malhas_regionais_ibge/RR_Municipios_2022/RR_Municipios_2022.shp', encoding='utf-8')\n",
    "    to = gpd.read_file('../data/malhas_regionais_ibge/TO_Municipios_2022/TO_Municipios_2022.shp', encoding='utf-8')\n",
    "\n",
    "    df_states = pd.concat([ac, am, ap, ma, mt, pa, ro, rr, to])\n",
    "\n",
    "    df_states.rename(columns={'CD_MUN':'GEOCODIBGE'}, inplace=True)\n",
    "\n",
    "    c_units = gpd.read_file('../data/conservation_units_legal_amazon/conservation_units_legal_amazon.shp',encoding='utf-8')\n",
    "    c_units.rename(columns={'nome':'UC'},inplace=True)\n",
    "    \n",
    "    return alerts_df, df_deter, legal_amazon, states, df_states, c_units\n",
    "\n",
    "alerts_df, df_deter, legal_amazon, states, df_states, c_units = load_data()\n",
    "folder_path = \"../Visualizations/DETER/Maps\"\n",
    "for file in os.listdir(folder_path):\n",
    "    os.remove(os.path.join(folder_path,file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bc1d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PA', 'MT', 'AM', 'RO', 'MA', 'TO', 'RR', 'AP', 'AC'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deter[\"UF\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f398fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSNAME        519\n",
      "VIEW_DATE        519\n",
      "AREAUCKM         519\n",
      "UC                26\n",
      "AREAMUNKM        519\n",
      "MUNICIPALI       519\n",
      "GEOCODIBGE       519\n",
      "UF               519\n",
      "ANO              519\n",
      "MES              519\n",
      "MES/ANO          519\n",
      "AREAMUNKM_SUM    519\n",
      "dtype: int64\n",
      "<DatetimeArray>\n",
      "['2018-01-21 00:00:00', '2018-02-16 00:00:00', '2018-01-30 00:00:00',\n",
      " '2018-02-19 00:00:00', '2018-02-22 00:00:00', '2018-02-27 00:00:00',\n",
      " '2016-11-27 00:00:00', '2018-02-28 00:00:00', '2018-03-03 00:00:00',\n",
      " '2018-05-23 00:00:00',\n",
      " ...\n",
      " '2018-05-13 00:00:00', '2018-05-14 00:00:00', '2024-09-27 00:00:00',\n",
      " '2018-01-04 00:00:00', '2018-05-28 00:00:00', '2018-04-06 00:00:00',\n",
      " '2018-02-11 00:00:00', '2018-04-15 00:00:00', '2018-01-09 00:00:00',\n",
      " '2018-01-01 00:00:00']\n",
      "Length: 519, dtype: datetime64[ms]\n"
     ]
    }
   ],
   "source": [
    "dfs_deter = {uf: df_deter[df_deter['UF'] == uf] for uf in df_deter['UF'].unique()}\n",
    "\n",
    "df_deter_PA = dfs_deter['PA']\n",
    "df_deter_MT = dfs_deter['MT']\n",
    "df_deter_AM = dfs_deter['AM']\n",
    "df_deter_RO = dfs_deter['RO']\n",
    "df_deter_MA = dfs_deter['MA']\n",
    "df_deter_TO = dfs_deter['TO']\n",
    "df_deter_RR = dfs_deter['RR']\n",
    "df_deter_AP = dfs_deter['AP']\n",
    "df_deter_AC = dfs_deter['AC']\n",
    "\n",
    "deter_states = ['PA', 'MT', 'AM', 'RO', 'MA', 'TO', 'RR', 'AP', 'AC']\n",
    "for s in deter_states:\n",
    "    var_name = f'df_deter_{s}'  # Dynamically create the variable name\n",
    "    grouped = globals()[var_name].groupby('VIEW_DATE', as_index=False)['AREAMUNKM'].sum()\n",
    "    globals()[var_name] = pd.merge(globals()[var_name], grouped, on='VIEW_DATE', suffixes=('', '_SUM'))\n",
    "    globals()[var_name] = globals()[var_name].drop_duplicates(subset=[\"VIEW_DATE\"], keep=\"first\")\n",
    "    \n",
    "print(df_deter_MA.count())\n",
    "print(df_deter_MA[\"VIEW_DATE\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIEW_DATE</th>\n",
       "      <th>CLASSNAME</th>\n",
       "      <th>AREAUCKM</th>\n",
       "      <th>UC</th>\n",
       "      <th>AREAMUNKM</th>\n",
       "      <th>MUNICIPALI</th>\n",
       "      <th>GEOCODIBGE</th>\n",
       "      <th>UF</th>\n",
       "      <th>ANO</th>\n",
       "      <th>MES</th>\n",
       "      <th>MES/ANO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>DESMATAMENTO_CR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FLORESTA NACIONAL DO JAMANXIM</td>\n",
       "      <td>0.116401</td>\n",
       "      <td>Senador Jose Porfirio</td>\n",
       "      <td>1507805</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>DEGRADACAO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FLORESTA NACIONAL DO JAMANXIM</td>\n",
       "      <td>2.575600</td>\n",
       "      <td>Novo Progresso</td>\n",
       "      <td>1505031</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-08</td>\n",
       "      <td>DESMATAMENTO_CR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.130197</td>\n",
       "      <td>Santana do Araguaia</td>\n",
       "      <td>1506708</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-11</td>\n",
       "      <td>DESMATAMENTO_CR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FLORESTA NACIONAL DE SARAC√Å-TAQUERA</td>\n",
       "      <td>0.238796</td>\n",
       "      <td>Almeirim</td>\n",
       "      <td>1500503</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-12</td>\n",
       "      <td>DESMATAMENTO_CR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.212494</td>\n",
       "      <td>Porto de Moz</td>\n",
       "      <td>1505908</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIEW_DATE        CLASSNAME  AREAUCKM                                   UC  \\\n",
       "0 2016-08-02  DESMATAMENTO_CR       0.0        FLORESTA NACIONAL DO JAMANXIM   \n",
       "1 2016-08-06       DEGRADACAO       0.0        FLORESTA NACIONAL DO JAMANXIM   \n",
       "2 2016-08-08  DESMATAMENTO_CR       0.0                                 None   \n",
       "3 2016-08-11  DESMATAMENTO_CR       0.0  FLORESTA NACIONAL DE SARAC√Å-TAQUERA   \n",
       "4 2016-08-12  DESMATAMENTO_CR       0.0                                 None   \n",
       "\n",
       "   AREAMUNKM             MUNICIPALI GEOCODIBGE  UF   ANO  MES  MES/ANO  \n",
       "0   0.116401  Senador Jose Porfirio    1507805  PA  2016    8  2016-08  \n",
       "1   2.575600         Novo Progresso    1505031  PA  2016    8  2016-08  \n",
       "2   0.130197    Santana do Araguaia    1506708  PA  2016    8  2016-08  \n",
       "3   0.238796               Almeirim    1500503  PA  2016    8  2016-08  \n",
       "4   0.212494           Porto de Moz    1505908  PA  2016    8  2016-08  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping by VIEW_DATE and summing AREAMUNKM for each state-specific DataFrame\n",
    "for state_name, state_df in dfs_deter.items():\n",
    "    # Group by VIEW_DATE and sum up AREAMUNKM\n",
    "    grouped = state_df.groupby('VIEW_DATE', as_index=False)['AREAMUNKM'].sum()\n",
    "    \n",
    "    # Merge the sum back into the state's DataFrame\n",
    "    dfs_deter[state_name] = pd.merge(state_df, grouped, on='VIEW_DATE', suffixes=('', '_SUM'))\n",
    "    dfs_deter=dfs_deter.drop_duplicates(subset = [\"VIEW_DATE\"],keep=\"first\")\n",
    "\n",
    "# Extract updated DataFrames for each state\n",
    "df_deter_PA = dfs_deter.get('PA', pd.DataFrame())\n",
    "df_deter_MT = dfs_deter.get('MT', pd.DataFrame())\n",
    "df_deter_AM = dfs_deter.get('AM', pd.DataFrame())\n",
    "df_deter_RO = dfs_deter.get('RO', pd.DataFrame())\n",
    "df_deter_MA = dfs_deter.get('MA', pd.DataFrame())\n",
    "df_deter_TO = dfs_deter.get('TO', pd.DataFrame())\n",
    "df_deter_RR = dfs_deter.get('RR', pd.DataFrame())\n",
    "df_deter_AP = dfs_deter.get('AP', pd.DataFrame())\n",
    "df_deter_AC = dfs_deter.get('AC', pd.DataFrame())\n",
    "\n",
    "df_deter_PA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119ddc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prathiba/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0684\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0383\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0447\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0290\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0339\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0288\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0334\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0267\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0363 \n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0261\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0388\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0415\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0501\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0504\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0409\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0227\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0348\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0436\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0214\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0350\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194814/3175462150.py:77: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  future_dates = pd.date_range(start=\"2025-01\", periods=12, freq='M')\n",
      "/home/prathiba/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0040\n",
      "Epoch 2/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027\n",
      "Epoch 3/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 4/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016  \n",
      "Epoch 5/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036\n",
      "Epoch 6/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074\n",
      "Epoch 7/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0052\n",
      "Epoch 8/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036\n",
      "Epoch 9/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.2526e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011    \n",
      "Epoch 11/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0011    \n",
      "Epoch 12/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0013  \n",
      "Epoch 13/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0029\n",
      "Epoch 14/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0011    \n",
      "Epoch 15/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011    \n",
      "Epoch 16/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0052\n",
      "Epoch 17/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027\n",
      "Epoch 18/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022\n",
      "Epoch 19/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032\n",
      "Epoch 20/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016  \n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194814/3175462150.py:77: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  future_dates = pd.date_range(start=\"2025-01\", periods=12, freq='M')\n",
      "/home/prathiba/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 3.9029e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.8609e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.9432e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.2263e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.2541e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 6.3336e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.4710e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.6863e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.5082e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.3659e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.5172e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.9310e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.1537e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.5111e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.7659e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.6442e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.4602e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1119e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.4193e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.5580e-04\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194814/3175462150.py:77: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  future_dates = pd.date_range(start=\"2025-01\", periods=12, freq='M')\n",
      "/home/prathiba/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0014\n",
      "Epoch 2/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0030\n",
      "Epoch 3/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0015\n",
      "Epoch 4/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0016\n",
      "Epoch 5/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0035\n",
      "Epoch 6/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0017\n",
      "Epoch 7/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0014\n",
      "Epoch 8/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.0876e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0015\n",
      "Epoch 10/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0015\n",
      "Epoch 11/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0027\n",
      "Epoch 12/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013\n",
      "Epoch 13/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0013\n",
      "Epoch 14/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0012\n",
      "Epoch 15/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0021\n",
      "Epoch 16/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0019\n",
      "Epoch 17/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0017\n",
      "Epoch 18/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0015\n",
      "Epoch 19/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 9.5370e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0014\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194814/3175462150.py:77: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  future_dates = pd.date_range(start=\"2025-01\", periods=12, freq='M')\n",
      "/home/prathiba/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 4.3021e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.8823e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.6268e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.9818e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.9566e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.8469e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.9901e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.3316e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.7088e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.4429e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.5286e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.1925e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.5653e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.3711e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.5319e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.3376e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.3105e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.5504e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m809/809\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.9196e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m 51/809\u001b[0m \u001b[32m‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 3.0672e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Predicting the next year's data\u001b[39;00m\n\u001b[1;32m     65\u001b[0m predicted \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/pra/GeorgiaTech/Fall24/CSE6242/Data_Analysis_Project/.environment/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import setuptools.dist\n",
    "\n",
    "# Prepare the data\n",
    "df_deter_PA.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, seq_length=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters\n",
    "seq_length = 12  # Using 12 months for yearly prediction\n",
    "future_predictions = []\n",
    "\n",
    "# Group by MUNICIPALI\n",
    "for municipali, group_data in df_deter_PA.groupby('MUNICIPALI'):\n",
    "    # Selecting relevant column for forecasting\n",
    "    data = group_data[['AREAMUNKM']].values\n",
    "\n",
    "    # Scaling the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Creating sequences\n",
    "    X, y = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "    if len(X) == 0:  # Skip municipalities with insufficient data\n",
    "        continue\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(100, activation='relu', input_shape=(seq_length, 1), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(100, activation='relu', return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, activation='relu', return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Training the model\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=16, verbose=1)\n",
    "\n",
    "    # Predicting the next year's data\n",
    "    predicted = []\n",
    "    last_sequence = data_scaled[-seq_length:]  # Last 12 months as input for next year's prediction\n",
    "\n",
    "    for _ in range(12):\n",
    "        next_value = model.predict(last_sequence[np.newaxis, :, :], verbose=0)\n",
    "        predicted.append(next_value[0, 0])\n",
    "        last_sequence = np.append(last_sequence[1:], next_value, axis=0)\n",
    "\n",
    "    # Inverse scaling to get actual values\n",
    "    predicted = scaler.inverse_transform(np.array(predicted).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Preparing the resulting DataFrame for the municipality\n",
    "    future_dates = pd.date_range(start=\"2025-01\", periods=12, freq='M')\n",
    "    future_df = pd.DataFrame({\n",
    "        'CLASSNAME': ['PREDICTED'] * 12,\n",
    "        'VIEW_DATE': future_dates,\n",
    "        'AREAUCKM': [0.0] * 12,\n",
    "        'UC': [None] * 12,\n",
    "        'AREAMUNKM': predicted,\n",
    "        'MUNICIPALI': [municipali] * 12,\n",
    "        'GEOCODIBGE': [group_data['GEOCODIBGE'].iloc[0]] * 12,\n",
    "        'UF': [group_data['UF'].iloc[0]] * 12,\n",
    "        'ANO': [2025] * 12,\n",
    "        'MES': list(range(1, 13)),\n",
    "        'MES/ANO': future_dates.strftime('%Y-%m')\n",
    "    })\n",
    "\n",
    "    future_predictions.append(future_df)\n",
    "\n",
    "# Combine all predictions into a single DataFrame\n",
    "final_predictions = pd.concat(future_predictions, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8ac31bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                FID             CLASSNAME QUADRANT PATH_ROW  VIEW_DATE SENSOR  \\\n",
      "0       100002_hist  CICATRIZ_DE_QUEIMADA     None   170105 2018-01-11   AWFI   \n",
      "1       100003_hist  CICATRIZ_DE_QUEIMADA     None   169105 2018-01-14   AWFI   \n",
      "2       100005_curr  CICATRIZ_DE_QUEIMADA     None   037017 2024-09-26    WFI   \n",
      "3       100005_hist      DESMATAMENTO_VEG     None   169105 2018-01-14   AWFI   \n",
      "4       100006_hist  CICATRIZ_DE_QUEIMADA     None   169105 2018-01-14   AWFI   \n",
      "...             ...                   ...      ...      ...        ...    ...   \n",
      "392804    9999_curr       DESMATAMENTO_CR     None   038016 2023-09-29    WFI   \n",
      "392805     999_curr  CICATRIZ_DE_QUEIMADA     None   036016 2023-08-16    WFI   \n",
      "392806      99_hist            DEGRADACAO        D   321074 2016-08-02  AWIFS   \n",
      "392807       9_curr       DESMATAMENTO_CR     None   036016 2023-08-01    WFI   \n",
      "392808       9_hist  CICATRIZ_DE_QUEIMADA        B   324078 2016-08-17  AWIFS   \n",
      "\n",
      "            SATELLITE  AREAUCKM    UC  AREAMUNKM          MUNICIPALI  \\\n",
      "0             CBERS-4       0.0  None   0.459839        Monte Alegre   \n",
      "1             CBERS-4       0.0  None   0.340975            Itaituba   \n",
      "2          AMAZONIA-1       0.0  None   1.373554         Marcelandia   \n",
      "3             CBERS-4       0.0  None   0.070781            Altamira   \n",
      "4             CBERS-4       0.0  None   0.149432            Itaituba   \n",
      "...               ...       ...   ...        ...                 ...   \n",
      "392804     AMAZONIA-1       0.0  None   0.206886  Candeias do Jamari   \n",
      "392805     AMAZONIA-1       0.0  None   0.425538  Sao Felix do Xingu   \n",
      "392806  RESOURCESAT-2       0.0  None  22.043149              Portel   \n",
      "392807     AMAZONIA-1       0.0  None   0.139690     Cumaru do Norte   \n",
      "392808  RESOURCESAT-2       0.0  None   7.582060         Paragominas   \n",
      "\n",
      "       GEOCODIBGE  UF                                           geometry  \\\n",
      "0         1504802  PA  POLYGON ((-54.04199 -1.90637, -54.04199 -1.906...   \n",
      "1         1503606  PA  POLYGON ((-55.27159 -6.43149, -55.27159 -6.432...   \n",
      "2         5105580  MT  POLYGON ((-53.91501 -11.20377, -53.91446 -11.2...   \n",
      "3         1500602  PA  POLYGON ((-53.69443 -6.16354, -53.69474 -6.163...   \n",
      "4         1503606  PA  POLYGON ((-55.27399 -6.44736, -55.27401 -6.447...   \n",
      "...           ...  ..                                                ...   \n",
      "392804    1100809  RO  POLYGON ((-63.0754 -8.60101, -63.07302 -8.5989...   \n",
      "392805    1507300  PA  POLYGON ((-52.15616 -6.23029, -52.15616 -6.228...   \n",
      "392806    1505809  PA  POLYGON ((-50.46904 -2.3053, -50.46901 -2.3052...   \n",
      "392807    1502764  PA  POLYGON ((-51.48065 -9.15754, -51.48063 -9.157...   \n",
      "392808    1505502  PA  POLYGON ((-46.80944 -2.5, -46.80973 -2.50014, ...   \n",
      "\n",
      "         ANO  MES  MES/ANO  \n",
      "0       2018    1  2018-01  \n",
      "1       2018    1  2018-01  \n",
      "2       2024    9  2024-09  \n",
      "3       2018    1  2018-01  \n",
      "4       2018    1  2018-01  \n",
      "...      ...  ...      ...  \n",
      "392804  2023    9  2023-09  \n",
      "392805  2023    8  2023-08  \n",
      "392806  2016    8  2016-08  \n",
      "392807  2023    8  2023-08  \n",
      "392808  2016    8  2016-08  \n",
      "\n",
      "[392808 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(alerts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2f53a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Deter                    CLASSNAME  VIEW_DATE  AREAUCKM    UC  AREAMUNKM  \\\n",
      "0       CICATRIZ_DE_QUEIMADA 2018-01-11       0.0  None   0.459839   \n",
      "1       CICATRIZ_DE_QUEIMADA 2018-01-14       0.0  None   0.340975   \n",
      "2       CICATRIZ_DE_QUEIMADA 2024-09-26       0.0  None   1.373554   \n",
      "3           DESMATAMENTO_VEG 2018-01-14       0.0  None   0.070781   \n",
      "4       CICATRIZ_DE_QUEIMADA 2018-01-14       0.0  None   0.149432   \n",
      "...                      ...        ...       ...   ...        ...   \n",
      "392804       DESMATAMENTO_CR 2023-09-29       0.0  None   0.206886   \n",
      "392805  CICATRIZ_DE_QUEIMADA 2023-08-16       0.0  None   0.425538   \n",
      "392806            DEGRADACAO 2016-08-02       0.0  None  22.043149   \n",
      "392807       DESMATAMENTO_CR 2023-08-01       0.0  None   0.139690   \n",
      "392808  CICATRIZ_DE_QUEIMADA 2016-08-17       0.0  None   7.582060   \n",
      "\n",
      "                MUNICIPALI GEOCODIBGE  UF   ANO  MES  MES/ANO  \n",
      "0             Monte Alegre    1504802  PA  2018    1  2018-01  \n",
      "1                 Itaituba    1503606  PA  2018    1  2018-01  \n",
      "2              Marcelandia    5105580  MT  2024    9  2024-09  \n",
      "3                 Altamira    1500602  PA  2018    1  2018-01  \n",
      "4                 Itaituba    1503606  PA  2018    1  2018-01  \n",
      "...                    ...        ...  ..   ...  ...      ...  \n",
      "392804  Candeias do Jamari    1100809  RO  2023    9  2023-09  \n",
      "392805  Sao Felix do Xingu    1507300  PA  2023    8  2023-08  \n",
      "392806              Portel    1505809  PA  2016    8  2016-08  \n",
      "392807     Cumaru do Norte    1502764  PA  2023    8  2023-08  \n",
      "392808         Paragominas    1505502  PA  2016    8  2016-08  \n",
      "\n",
      "[392808 rows x 11 columns]\n",
      "\n",
      "Legal Amazon         sprarea    sprperimet sprclasse  \\\n",
      "0  5.068004e+12  1.877763e+07  AM_Legal   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-44.04149 -2.46528, -43.95183 -6.754...  \n",
      "\n",
      "States    id         nome sigla geocodigo  \\\n",
      "0   2     Rond√¥nia    RO        11   \n",
      "1   3         Acre    AC        12   \n",
      "2   4     Amazonas    AM        13   \n",
      "3   5      Roraima    RR        14   \n",
      "4   6         Par√°    PA        15   \n",
      "5   7        Amap√°    AP        16   \n",
      "6   8    Tocantins    TO        17   \n",
      "7  14  Mato Grosso    MT        51   \n",
      "8  26     Maranh√£o    MA        21   \n",
      "\n",
      "                                            geometry  \n",
      "0  MULTIPOLYGON (((-60.71438 -13.69188, -60.72165...  \n",
      "1  MULTIPOLYGON (((-66.62515 -9.9029, -66.62446 -...  \n",
      "2  MULTIPOLYGON (((-73.78698 -7.09645, -73.73817 ...  \n",
      "3  MULTIPOLYGON (((-63.37171 2.21232, -63.37086 2...  \n",
      "4  MULTIPOLYGON (((-58.89539 1.2283, -58.895 1.22...  \n",
      "5  MULTIPOLYGON (((-54.87194 2.43435, -54.87178 2...  \n",
      "6  POLYGON ((-45.93767 -10.25589, -45.87883 -10.2...  \n",
      "7  MULTIPOLYGON (((-50.50981 -12.86237, -50.50174...  \n",
      "8  MULTIPOLYGON (((-44.04109 -2.48442, -44.03955 ...  \n"
     ]
    }
   ],
   "source": [
    "print(\"DF Deter\", df_deter)\n",
    "print(\"\\nLegal Amazon\", legal_amazon)\n",
    "print(\"\\nStates\", states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9931b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Municipalities     GEOCODIBGE          NM_MUN SIGLA_UF  AREA_KM2  \\\n",
      "0      1200013      Acrel√¢ndia       AC  1811.613   \n",
      "1      1200054    Assis Brasil       AC  4979.073   \n",
      "2      1200104       Brasil√©ia       AC  3928.174   \n",
      "3      1200138          Bujari       AC  3034.869   \n",
      "4      1200179        Capixaba       AC  1705.824   \n",
      "..         ...             ...      ...       ...   \n",
      "134    1721208  Tocantin√≥polis       TO  1083.600   \n",
      "135    1721257        Tupirama       TO   706.883   \n",
      "136    1721307      Tupiratins       TO   889.126   \n",
      "137    1722081    Wanderl√¢ndia       TO  1365.431   \n",
      "138    1722107         Xambio√°       TO  1190.489   \n",
      "\n",
      "                                              geometry  \n",
      "0    POLYGON ((-67.07612 -10.08798, -67.07659 -10.0...  \n",
      "1    POLYGON ((-69.55253 -10.87353, -69.52086 -10.8...  \n",
      "2    POLYGON ((-68.75712 -11.01097, -68.75752 -11.0...  \n",
      "3    POLYGON ((-67.92167 -9.69355, -67.91736 -9.693...  \n",
      "4    POLYGON ((-67.73403 -10.71177, -67.73414 -10.7...  \n",
      "..                                                 ...  \n",
      "134  POLYGON ((-47.39135 -6.22329, -47.38537 -6.230...  \n",
      "135  POLYGON ((-48.17812 -8.95212, -48.17816 -8.952...  \n",
      "136  POLYGON ((-48.21831 -8.54589, -48.21842 -8.546...  \n",
      "137  POLYGON ((-48.12918 -6.81092, -48.12918 -6.810...  \n",
      "138  POLYGON ((-48.57962 -6.62613, -48.57963 -6.626...  \n",
      "\n",
      "[808 rows x 5 columns]\n",
      "\n",
      "C Units        id                                                 UC  \\\n",
      "0     366                     ESTA√á√ÉO ECOL√ìGICA DO GR√ÉO PAR√Å   \n",
      "1      30             RESERVA EXTRATIVISTA RIO PRETO-JACUND√Å   \n",
      "2      40                RESERVA EXTRATIVISTA DO M√©DIO PUR√∫S   \n",
      "3     789      √ÅREA DE PROTE√á√ÉO AMBIENTAL - BAIXO RIO BRANCO   \n",
      "4      44  RESERVA DE DESENVOLVIMENTO SUSTENTAVEL DO RIO ...   \n",
      "..    ...                                                ...   \n",
      "353   276           PARQUE NACIONAL DA CHAPADA DOS GUIMAR√ÉES   \n",
      "354  1625  √ÅREA DE PROTE√á√ÉO AMBIENTAL MUNICIPAL DO ARIC√Å-A√áU   \n",
      "355   294                    PARQUE ESTADUAL √ÅGUAS DO CUIAB√Å   \n",
      "356   356                 RESERVA EXTRATIVISTA LAGO DO CEDRO   \n",
      "357  1833  RESERVA PARTICULAR DO PATRIM√îNIO NATURAL POLEI...   \n",
      "\n",
      "                                    categoria grupo     esfera ano_cria  \\\n",
      "0                           Esta√ß√£o Ecol√≥gica    PI   estadual     2006   \n",
      "1                        Reserva Extrativista    US   estadual     1996   \n",
      "2                        Reserva Extrativista    US    federal     2008   \n",
      "3                  √Årea de Prote√ß√£o Ambiental    US   estadual     2006   \n",
      "4      Reserva de Desenvolvimento Sustent√°vel    US   estadual     2006   \n",
      "..                                        ...   ...        ...      ...   \n",
      "353                                    Parque    PI    federal     1989   \n",
      "354                √Årea de Prote√ß√£o Ambiental    US  municipal     1999   \n",
      "355                                    Parque    PI   estadual     2002   \n",
      "356                      Reserva Extrativista    US    federal     2006   \n",
      "357  Reserva Particular do Patrim√¥nio Natural    US   estadual     1998   \n",
      "\n",
      "                                              geometry  \n",
      "0    POLYGON ((-58.8938 1.2278, -58.87546 1.22389, ...  \n",
      "1    POLYGON ((-62.19898 -8.85976, -62.20689 -8.876...  \n",
      "2    POLYGON ((-64.89972 -7.13722, -64.89886 -7.138...  \n",
      "3    POLYGON ((-60.96747 0.82676, -60.96742 0.82679...  \n",
      "4    MULTIPOLYGON (((-61.189 -5.44252, -61.1891 -5....  \n",
      "..                                                 ...  \n",
      "353  POLYGON ((-55.92403 -15.2156, -55.92343 -15.21...  \n",
      "354  POLYGON ((-55.90371 -15.44614, -55.87506 -15.5...  \n",
      "355  POLYGON ((-55.30824 -14.31614, -55.3196 -14.35...  \n",
      "356  MULTIPOLYGON (((-50.99498 -14.61956, -50.9951 ...  \n",
      "357  MULTIPOLYGON (((-56.16939 -17.19256, -56.17658...  \n",
      "\n",
      "[358 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"DF Municipalities\", df_states)\n",
    "print(\"\\nC Units\", c_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "11a5f7a6-cb80-42df-97a8-1c9deb3823a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING TEXTS (ENGLISH AND PORTUGUESE)\n",
    "\n",
    "df_texts = pd.read_csv('../texts/texts_deter.csv', sep='¬ß', engine='python')\n",
    "english = {list(df_texts['Key'])[i]: list(df_texts['English'])[i] for i in range(len(list(df_texts['Key'])))}\n",
    "# portuguese = {list(df_texts['Key'])[i]: list(df_texts['Portuguese'])[i] for i in range(len(list(df_texts['Key'])))}\n",
    "\n",
    "classes_deter_en = {'CICATRIZ_DE_QUEIMADA': 'Forest Fire Scar',\n",
    "          'DESMATAMENTO_CR': 'Deforestation with Exposed Soil',\n",
    "          'DESMATAMENTO_VEG': 'Deforestation with Vegetation',\n",
    "          'MINERACAO': 'Mining',\n",
    "          'DEGRADACAO': 'Degradation',\n",
    "          'CS_DESORDENADO': 'Selective Logging Type 1 (Disordered)',\n",
    "          'CS_GEOMETRICO': 'Selective Logging Type 2 (Geometric)',\n",
    "}\n",
    "\n",
    "states_dict = {\n",
    "    \"MT\": \"Mato Grosso\",\n",
    "    \"PA\": \"Par√°\",\n",
    "    \"AM\": \"Amazonas\",\n",
    "    \"RO\": \"Rond√¥nia\",\n",
    "    \"MA\": \"Maranh√£o\",\n",
    "    \"RR\": \"Roraima\",\n",
    "    \"AC\": \"Acre\",\n",
    "    \"TO\": \"Tocantins\",\n",
    "    \"AP\": \"Amap√°\"\n",
    "}\n",
    "\n",
    "def get_texts():\n",
    "    return classes_deter_en, english\n",
    "\n",
    "\n",
    "\n",
    "dict_classes, texts = get_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2333859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CICATRIZ_DE_QUEIMADA': 'Forest Fire Scar', 'DESMATAMENTO_CR': 'Deforestation with Exposed Soil', 'DESMATAMENTO_VEG': 'Deforestation with Vegetation', 'MINERACAO': 'Mining', 'DEGRADACAO': 'Degradation', 'CS_DESORDENADO': 'Selective Logging Type 1 (Disordered)', 'CS_GEOMETRICO': 'Selective Logging Type 2 (Geometric)'}\n"
     ]
    }
   ],
   "source": [
    "print(dict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bd430262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_title': 'Amazon Deforestation Monitor', 'date_format': '%m-%d-%Y', 'date_format2': '%m-%Y', 'date_format3': 'MM/DD/YYYY', 'deter_expander_title': 'What is DETER?', 'deter_expander_desc_1': 'The [DETER](http://www.inpe.br/cra/projetos_pesquisas/deter.php) system, short for \"Real-Time Deforestation Detection\", is a system developed by the Brazilian National Institute for Space Research (INPE), aimed at monitoring and identifying changes in forest cover within the Legal Amazon in Brazil. This system is essential for environmental enforcement and combating illegal deforestation, providing crucial data for prevention and control actions.\\nThrough high-resolution satellite images, DETER can detect areas of deforestation and forest degradation with high precision and almost in real-time. This information is made available periodically, allowing for a swift response from environmental authorities.\\n\\nDETER\\'s detections are classified into various categories, reflecting different types of vegetation changes and land use. Among the main alert classes issued by the system are:\\n- ***Forest Fire Scar***: Areas affected by fires, leaving visible marks on the vegetation.\\n- ***Deforestation with Exposed Soil***: Areas where the vegetation has been completely removed, exposing the soil.\\n- ***Deforestation with Vegetation***: Partially deforested areas, where there is still the presence of remaining vegetation.\\n- ***Mining***: Areas impacted by mineral extraction, usually characterized by large pits or tailings piles.\\n- ***Degradation***: Areas where the vegetation has been degraded but not fully removed. This can include selective logging of trees or the reduction of forest density.\\n- ***Selective Logging Type 1 (Disordered) and Type 2 (Geometric)***: Areas where selective tree extraction has occurred. \"Type 1\" refers to cuts made in a disordered manner, while \"Type 2\" indicates extraction with defined geometric patterns.\\n', 'deter_expander_desc_2': '\\nThrough continuous monitoring and detailed classification of observed changes, DETER plays a crucial role in preserving the Amazon, offering a powerful tool for understanding and combating deforestation in the region.', 'title_deter_graph1': 'Classes by Affected Area', 'about_deter_alerts': '#### About DETER alerts between {} and {}', 'deter_metric1_label': 'Total Alerts:', 'title_deter_graph2': 'Alert Occurrences by Class', 'title_deter_graph3': 'Most Affected States by Damaged Area', 'title_deter_graph4': 'Most Affected Cities by Damaged Area', 'graph4_desc': 'The list highlights a predominance of cities in the states of Par√° and Mato Grosso, indicating regions of high pressure on the forest, whether due to deforestation, fires, or other forms of environmental degradation. Labrea in Amazonas and Porto Velho in Rond√¥nia also feature among the most affected, highlighting the extent of the deforestation problem across the entire Legal Amazon region.\\n\\nFeliz Natal and Quer√™ncia, both in Mato Grosso, as well as Novo Progresso and Paragominas in Par√°, are examples of other cities with significantly impacted areas. This reflects the urgency of actions aimed at conservation and sustainable management of natural resources in the region, emphasizing the need for continuous and effective monitoring to combat deforestation and preserve biodiversity.', 'title_deter_graph5': 'Damage per Month in the Legal Amazon (km¬≤)', 'title_deter_graph6': 'Monthly Total Damage in the Legal Amazon: Analysis from {} to {}', 'labels_deter_graph6': \"['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\", 'title_deter_graph7': 'Trend Analysis: Annual Variations of Damages', 'about': 'About DETER', 'general': 'General Vision', 'alert_classes': 'Alert Classes', 'states_statistics': 'States', 'cities_statistics': 'Cities', 'ucs_statistics': 'Conservation Units', 'dmg_ty': 'Damage Through Years', 'map': 'Map View', 'graph3_desc': '- **Mato Grosso and Par√°** lead in terms of affected area, with 75,043 km¬≤ and 64,342 km¬≤ respectively. This highlights the intensity of deforestation and degradation in these states, possibly due to agricultural activities, logging, and mining.\\n- **Amazonas, Rond√¥nia, and Maranh√£o** also show significant affected areas, reflecting the extent of the deforestation problem in the region.\\n- **Roraima, Acre, Tocantins, and Amap√°** display smaller affected areas but still face significant challenges in terms of conservation and sustainable resource management.', 'graphs_12_desc': '- **Deforestation with Exposed Soil**: With the highest number of alerts (234,632), this class significantly impacts the environment, exposing 51,420 km¬≤ of soil. This extensive clearing of vegetation for agriculture or other land uses directly contributes to habitat loss, biodiversity reduction, and increased carbon emissions.\\n\\n- **Forest Fire Scar**: Representing the second highest alert occurrence (66,510) and the largest affected area (95,351 km¬≤), forest fire scars indicate areas impacted by fires. These events can be natural or human-induced, leading to substantial ecological and economic losses.\\n\\n- **Degradation**: With 32,273 occurrences and an affected area of 20,064 km¬≤, degradation  reflect areas where vegetation has been damaged or partially removed but not entirely destroyed. This process can be caused by unsustainable land management practices, including logging and slash-and-burn agriculture.\\n\\n- **Selective Logging Type 1 and Type 2**: These classes highlight specific logging activities, affecting areas of 13,482 km¬≤ and 10,566 km¬≤, respectively. Although less extensive than outright deforestation, selective logging can still lead to forest fragmentation and degradation.\\n\\n- **Deforestation with Vegetation**: This class accounts for 8,805 alerts and impacts a relatively smaller area of 2,133 km¬≤. It represents areas where deforestation has occurred but some vegetation remains, indicating either partial clearance or the initial stages of deforestation.\\n\\n- **Mining**: The least extensive in terms of area (691 km¬≤) but noteworthy for its environmental impact, mining activities lead to significant local damage, including habitat destruction, water pollution, and soil erosion.', 'graph8_title': 'Regional Analysis of Impact Distribution by Alert Class in the Legal Amazon', 'graph8_desc': '**Forest Fire Scar** is predominantly the class with the largest affected area in almost all states, highlighting the recurrence and extent of forest fires in the region. Mato Grosso and Par√° lead with the largest areas affected, indicating high vulnerability to fires.\\n\\n**Deforestation with Exposed Soil** has a significant number of occurrences, with Par√° showing the largest area affected by this class. This signals intense deforestation activity, where vegetation is completely removed, exposing the soil.\\n\\nClasses related to **Selective Logging (Type 1 and Type 2)** show a more heterogeneous distribution, with Mato Grosso presenting the largest area affected. This may reflect specific forest management practices or logging activities in the region.\\n\\n**Degradation** is observed in all states but with relatively smaller affected areas compared to other classes. This suggests that, although present, degradation may occur on a less extensive scale than total deforestation or forest fires.\\n\\n**Mining**, although representing the smallest area affected among the classes, is notable in states like Par√°. The presence of mining activities, even on a smaller scale, highlights the diversity of land pressures in the Legal Amazon.', 'graph5_desc': \"The line chart above illustrates the monthly impact in the Legal Amazon, highlighting the area affected by deforestation and other disturbances reported through the DETER system from 2016 to 2024. Each point on the graph represents the total area affected in square kilometers for that month. The markers highlight the months with the highest impact within each year, visually emphasizing the periods of greatest environmental stress.\\n\\nThis analysis offers a temporal perspective on deforestation trends, with the marked points indicating the peak periods of deforestation activity. Such peaks often correlate with dry seasons or periods of increased land use change activities. The chart's time range allows for an observation of how deforestation patterns evolve, potentially influenced by policy changes, enforcement levels, or economic factors.\\n\\nBy aggregating data monthly, we can identify specific times of the year that consistently show higher levels of deforestation, guiding targeted interventions. It also underscores the importance of continuous monitoring and the need for effective strategies to mitigate these impacts. This approach not only aids in understanding the dynamics of deforestation but also serves as a critical tool for policymakers, conservationists, and researchers striving to preserve the Amazon rainforest's biodiversity and ecological integrity.\", 'graph6_desc': \"This bar chart visualizes the total damage per month in the Legal Amazon, aggregating data across all complete years in the dataset. It reveals a distinct seasonal pattern in deforestation and degradation activities, with notable peaks and troughs throughout the year.\\n\\nFrom January through December, the data show a progressive increase in the area affected, peaking significantly in the months of July, August and September. This period corresponds to the dry season in the Amazon, when the risk of fires is heightened, and land-clearing activities for agriculture or illegal logging tend to increase due to the lower moisture levels.\\n\\nThe chart shows a sharp decline in October, although the figures remain high at 24,590 km¬≤, suggesting that while the peak of the deforestation season may have passed, significant impacts continue into the latter part of the year. Following October, there's a notable reduction in activity, with November and December showing a considerable drop in the area affected, aligning with the onset of the rainy season, which typically discourages deforestation and burning activities.\\n\\nThis seasonal trend underscores the critical need for targeted intervention strategies during the dry season to mitigate the highest rates of deforestation and degradation. Understanding these patterns is crucial for policymakers, environmental agencies, and conservation groups in planning and implementing measures to protect the Amazon rainforest effectively.\", 'graph7_desc': \"This visualization provides a detailed annual analysis of changes in the damaged areas within the Legal Amazon, focusing exclusively on complete years within the dataset. This approach ensures that the data presented are comprehensive and representative of each year's full impact, thereby allowing for accurate year-over-year comparisons. Each bar represents the total area affected in a particular year. The use of arrows to illustrate changes between the years adds a dynamic element to the visualization, with green arrows denoting reductions in affected areas‚Äîsuggesting periods of recovery or effective conservation measures‚Äîand red arrows highlighting increases, indicative of heightened deforestation activities.</br></br>The visualization commences in 2017, establishing a foundational comparison point for the years that follow. Thereafter, the data reveal a series of fluctuations indicative of the region's ongoing struggle with deforestation. The observed decrease in 2018 may suggest the impact of effective conservation strategies or advantageous environmental conditions. In contrast, the subsequent increase in 2019, reaching a notable peak in 2020, followed by a pronounced decline in 2021, and yet another ascent in 2022, underscore the persistent challenges in combating deforestation. The year 2023 continues this pattern, displaying a decrease in affected areas, which might be attributed to a combination of factors including policy implementation, enforcement rigor, economic variables, and environmental conditions that collectively determine the susceptibility of the Amazon to deforestation and degradation activities. This chronological sequence of data points to the complex dynamics at play, highlighting the necessity for adaptive and robust approaches to preservation and management of this critical ecosystem.\", 'graph9_title': 'Most Affected Conservation Units by Damaged Area', 'graph9_desc': 'Conservation Units (UCs) are legally protected natural areas designated to preserve biodiversity, protect endangered species, maintain and restore essential ecosystems, and promote sustainable development. They play a crucial role in mitigating climate change, maintaining water resources, and preserving natural landscapes.\\n\\nThe chart highlights the top 25 UCs in the Legal Amazon most affected by deforestation and degradation, measured by the impacted area in square kilometers, showcasing the environmental pressures these protected areas face. The UCs range from National Forests, like the Jamanxim National Forest leading with an impacted area of 1,040 km¬≤, to Extractive Reserves, like the Chico Mendes Extractive Reserve, and National Parks, like the Araguaia National Park.', 'classes_graphs_title': 'Impact Analysis of Alert Classes in the Amazon Deforestation', 'total_dmg': 'Total Damaged Area:', 'vis_type': 'Choose Visualization Type:', 'vis_options': 'None;States;Cities;Conservation Units', 'vis_state': 'Choose the state you want to visualize on the map:', 'git': 'Github repository', 'inpe_ref': 'NATIONAL INSTITUTE FOR SPACE RESEARCH. GENERAL COORDINATION FOR EARTH OBSERVATION. AMAZON AND OTHER BIOMES MONITORING PROGRAM. Notices ‚Äì Legal Amazon ‚Äì Available at: <a href=\"https://terrabrasilis.dpi.inpe.br/downloads/\">TerraBrasilis</a>. Accessed on: July 10, 2024.'}\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9d8d531d-c317-482b-9b39-c6cea77d1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(geo_df, mode=2, crs='EPSG:31982'):\n",
    "\n",
    "    if mode==0:\n",
    "        centroids = geo_df.copy()\n",
    "        centroids[\"centroid\"] = centroids.geometry.centroid\n",
    "        centroids[\"latitude\"] = centroids.centroid.y\n",
    "        centroids[\"longitude\"] = centroids.centroid.x\n",
    "        return centroids\n",
    "\n",
    "    if mode==1:\n",
    "        geo_df_proj = geo_df.copy()\n",
    "        geo_df_proj.to_crs(crs)\n",
    "        \n",
    "        geo_df_proj['centroid'] = geo_df_proj.geometry.centroid\n",
    "        geo_df_proj['latitude'] = geo_df_proj.centroid.y\n",
    "        geo_df_proj['longitude'] = geo_df_proj.centroid.x\n",
    "        \n",
    "        centroids = gpd.GeoDataFrame(geo_df_proj, geometry='centroid', crs=crs)\n",
    "    \n",
    "        centroids = centroids.to_crs(geo_df.crs)\n",
    "        return centroids\n",
    "\n",
    "    if mode==2:\n",
    "        df = geo_df.copy()\n",
    "        df['representative_point'] = df.geometry.representative_point()\n",
    "        df['latitude'] = df['representative_point'].apply(lambda p: p.y)\n",
    "        df['longitude'] = df['representative_point'].apply(lambda p: p.x)\n",
    "        return df\n",
    "\n",
    "def folium_map_init():\n",
    "    map = folium.Map(location=[-7.25, -60], zoom_start=4)\n",
    "    return map\n",
    "\n",
    "def folium_add_markers(container, df_data, geo_df, get_centroid_mode, df_deter, key, popup_title_column, popup_total_area_text='√Årea Total Afetada:',total_area_column='AREAMUNKM'):\n",
    "\n",
    "    all_classes = sorted(df_deter['CLASSNAME'].unique())\n",
    "    df_centroids = get_centroids(geo_df,get_centroid_mode)\n",
    "    \n",
    "    for idx, row in df_data.iterrows():\n",
    "\n",
    "        coords = df_centroids.loc[df_centroids[key] == row[key]].iloc[0]\n",
    "\n",
    "        info = ''\n",
    "        \n",
    "        if row['AREAMUNKM']>0:\n",
    "            df_stats = df_deter[df_deter[key] == row[key]]\n",
    "            df_stats_summed = df_stats.groupby('CLASSNAME')['AREAMUNKM'].sum().reset_index()\n",
    "            df_stats_complete = pd.DataFrame({'CLASSNAME': all_classes})\n",
    "            df_stats_complete = df_stats_complete.merge(df_stats_summed, on='CLASSNAME', how='left').fillna(0)\n",
    "            df_stats_complete['DESC'] = df_stats_complete['CLASSNAME'].map(dict_classes)\n",
    "            df_stats_complete = df_stats_complete.sort_values(by='AREAMUNKM', ascending=False)\n",
    "    \n",
    "            total = df_stats_complete['AREAMUNKM'].sum()\n",
    "            \n",
    "            # Calculates percentage of every class\n",
    "            for ind, lin in df_stats_complete.iterrows():\n",
    "                perc = (lin['AREAMUNKM'] * 100) / total\n",
    "                info += f\"{lin['DESC']}: {lin['AREAMUNKM']:.0f} km¬≤ ({perc:.2f}%)<br>\"\n",
    "        \n",
    "        popup_text = f\"\"\"\n",
    "        <div style='white-space: nowrap;'>\n",
    "        <span style='font-size: 16px; font-weight: bold;'>{row[popup_title_column]}</span><br><br> {popup_total_area_text} {row[total_area_column]:.0f} km¬≤<br><br> {info}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "        # Add marker on Map or MarkerCluster (container)\n",
    "        folium.Marker(\n",
    "            location=[coords['latitude'], coords['longitude']],\n",
    "            popup=popup_text,\n",
    "            icon=folium.Icon(color='red', icon='triangle-exclamation', prefix='fa')\n",
    "        ).add_to(container)\n",
    "    \n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3be69-e697-4932-a084-88cf75ccda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_map(file_name,map):\n",
    "    map.save(f\"../Visualizations/DETER/Maps/{file_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f0f33-fdc8-4242-81e1-e0e6f65a2e53",
   "metadata": {},
   "source": [
    "### States Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "99ba5eba-f044-46e4-afd8-13f7816e25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_map():\n",
    "    ############# Data Preparation #############\n",
    "    # Load shapefiles for regions (if not already loaded)\n",
    "    # legal_amazon = gpd.read_file('data/brazilian_legal_amazon/brazilian_legal_amazon.shp', encoding='utf-8')\n",
    "    # states = gpd.read_file('data/states_legal_amazon/states_legal_amazon.shp', encoding='utf-8')\n",
    "    \n",
    "    df_deter = alerts_df.copy()\n",
    "    gb_uf = df_deter.groupby('UF')['AREAMUNKM'].sum().sort_values(ascending=False)\n",
    "    gb_uf = pd.DataFrame(gb_uf)\n",
    "    gb_uf['NOME_ESTADO'] = gb_uf.index.map(states_dict)\n",
    "    gb_uf['NOME_SIGLA'] = gb_uf['NOME_ESTADO'] + ' (' + gb_uf.index + ')' \n",
    "    gb_uf = gb_uf.reset_index()\n",
    "\n",
    "    states_copy = states.copy()\n",
    "    states_copy = states_copy.rename(columns={'sigla': 'UF'})\n",
    "\n",
    "    #############       Folium       #############\n",
    "    map = folium_map_init()\n",
    "\n",
    "    # Customizing state borders color\n",
    "    style_states = {'fillOpacity': 0.3, 'color': '#005f73', 'weight': 2}\n",
    "    folium.GeoJson(states_copy, name='States', style_function=lambda x: style_states).add_to(map)\n",
    "\n",
    "    # Customizing Legal Amazon border color\n",
    "    style_legal_amazon = {'fillOpacity': 0, 'color': '#0a9396', 'weight': 3}\n",
    "    folium.GeoJson(legal_amazon, name='Legal Amazon', style_function=lambda x: style_legal_amazon).add_to(map)\n",
    "\n",
    "    # Customizing Choropleth color scheme\n",
    "    folium.Choropleth(\n",
    "        geo_data=states_copy,\n",
    "        data=gb_uf,\n",
    "        columns=['UF', 'AREAMUNKM'],\n",
    "        key_on='feature.properties.UF',\n",
    "        fill_color='Reds',\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        nan_fill_color='white',\n",
    "        bins=8,\n",
    "        highlight=True,\n",
    "        legend_name='Affected Area in km¬≤',\n",
    "        name='Most Affected States'\n",
    "    ).add_to(map)\n",
    "\n",
    "    # Adding markers with popups for more information\n",
    "    map = folium_add_markers(map, gb_uf, states_copy, 1, df_deter, 'UF', 'NOME_SIGLA', texts['total_dmg'], 'AREAMUNKM')\n",
    "\n",
    "    # Adding layer control\n",
    "    folium.LayerControl().add_to(map)\n",
    "\n",
    "    return map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "85038168-9081-41c7-9e4d-3d8d9dddcabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/1685801861.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  geo_df_proj['centroid'] = geo_df_proj.geometry.centroid\n",
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/1685801861.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  geo_df_proj['latitude'] = geo_df_proj.centroid.y\n",
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/1685801861.py:16: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  geo_df_proj['longitude'] = geo_df_proj.centroid.x\n"
     ]
    }
   ],
   "source": [
    "dict_classes, texts = get_texts()\n",
    "map = states_map()\n",
    "save_map('States_EN',map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb6ddb-1ef6-4f39-a29a-32d74b5898c4",
   "metadata": {},
   "source": [
    "### Cities Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c0307c5c-56cb-45a7-a8b0-3f928c05ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cities_map(filter=[]):\n",
    "    \n",
    "    ############# Data Preparation #############\n",
    "\n",
    "    geocodibge = alerts_df.drop_duplicates(subset='MUNICIPALI').set_index('MUNICIPALI')['GEOCODIBGE']\n",
    "    sum_areamunkm = alerts_df.groupby('MUNICIPALI')['AREAMUNKM'].sum().reset_index()\n",
    "    sum_areamunkm['GEOCODIBGE'] = sum_areamunkm['MUNICIPALI'].map(geocodibge)\n",
    "\n",
    "    merge = pd.merge(df_states, sum_areamunkm, on='GEOCODIBGE', how='left')\n",
    "\n",
    "    if len(filter)>0:\n",
    "        merge = merge[merge['SIGLA_UF'].isin(filter)]\n",
    "    \n",
    "    # merge[merge['AREAMUNKM'].isna()]\n",
    "    # 226 cidades n√£o cont√©m avisos. Esses valores ausentes ser√£o preenchidos com 0.\n",
    "    merge['AREAMUNKM'].fillna(0, inplace=True)\n",
    "\n",
    "    #############       Folium       #############\n",
    "    map = folium_map_init()\n",
    "\n",
    "    style_cities = {'fillOpacity':0 ,'color' : '#117306', 'weight': 1}\n",
    "    folium.GeoJson(merge, name = 'Cities', style_function= lambda x: style_cities).add_to(map)\n",
    "\n",
    "    style_states = {'fillOpacity':0 ,'color' : '#117306', 'weight': 2}\n",
    "    folium.GeoJson(states, name = 'States', style_function= lambda x: style_states).add_to(map)\n",
    "    \n",
    "    style_legal_amazon = {'fillOpacity':0 ,'color' : '#117306', 'weight': 3}\n",
    "    folium.GeoJson(legal_amazon, name = 'Legal Amazon', style_function= lambda x: style_legal_amazon).add_to(map)\n",
    "    \n",
    "    folium.Choropleth(geo_data=merge.to_json(),\n",
    "                  name='Choropleth',\n",
    "                  data=merge,\n",
    "                  columns=['GEOCODIBGE', 'AREAMUNKM'],\n",
    "                  key_on = 'feature.properties.GEOCODIBGE',\n",
    "                  fill_color = 'Reds',\n",
    "                  nan_fill_color = 'white',\n",
    "                  highlight = True,\n",
    "                  legend_name='Affected Area in km¬≤').add_to(map)\n",
    "\n",
    "    marker_cluster = MarkerCluster().add_to(map)\n",
    "    marker_cluster = folium_add_markers(marker_cluster, merge, merge, 2, alerts_df, 'GEOCODIBGE', 'NM_MUN', texts['total_dmg'], 'AREAMUNKM')\n",
    "    folium.LayerControl().add_to(map)\n",
    "\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "727d2738-73ab-4205-971d-70a8c3d4e866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AC', 'AM', 'AP', 'MA', 'MT', 'PA', 'RO', 'RR', 'TO']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_states = list(df_states['SIGLA_UF'].unique())\n",
    "lst_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ae0d49a4-ffd3-46ff-b3c0-d225f5220c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_AC saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_AM saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_AP saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_MA saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_MT saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_PA saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_RO saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_RR saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities_EN_TO saved.\n"
     ]
    }
   ],
   "source": [
    "dict_classes, texts = get_texts()\n",
    "\n",
    "for i in range(len(lst_states)):\n",
    "    filter = []\n",
    "    filter.append(lst_states[i])\n",
    "    map = cities_map(filter)\n",
    "    map_name = 'Cities_EN_' + filter[0]\n",
    "    save_map(map_name,map)\n",
    "    print(map_name + ' saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "424a5566-2f54-439a-90b7-99a1234fd930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/np6rblqn5jzdcb48rl2zt3bm0000gn/T/ipykernel_37544/263644965.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merge['AREAMUNKM'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_Cities_EN saved.\n"
     ]
    }
   ],
   "source": [
    "dict_classes, texts = get_texts()\n",
    "map = cities_map([])\n",
    "map_name = 'All_Cities_EN'\n",
    "save_map(map_name,map)\n",
    "print(map_name + ' saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168d114-5aa4-4aa4-9cac-fbc887230fb3",
   "metadata": {},
   "source": [
    "### C Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "de5215ef-ecc2-4d40-99a7-4d706194695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    s = s.upper()\n",
    "    return s\n",
    "\n",
    "def c_units_map():\n",
    "    ############# Data Preparation #############\n",
    "    c_units_copy = c_units.copy()\n",
    "    c_units_copy['UC'] = c_units_copy['UC'].apply(normalize_string)\n",
    "    \n",
    "    alerts_uc = alerts_df[alerts_df['UC'].notna()].copy()\n",
    "    dic_correcao = {'FLORESTA NACIONAL DE ALTAMIRA': 'FLORESTA NACIONAL ALTAMIRA', \n",
    "                    'FLORESTA NACIONAL DE CAXIUAN√Ç': 'FLORESTA NACIONAL DE CAXIUAN√É', \n",
    "                    'FLORESTA NACIONAL DO AMANA': 'FLORESTA NACIONAL DO AMAN√Å',\n",
    "                    'FLORESTA NACIONAL DO BOM FUTURO': 'FLORESTA NACIONAL DE BOM FUTURO',\n",
    "                    'FLORESTA NACIONAL DO ITACAIUNAS': 'FLORESTA NACIONAL DE ITACAIUNAS',\n",
    "                    'FLORESTA NACIONAL DO JATUARANA': 'FLORESTA NACIONAL DE JATUARANA',\n",
    "                    'FLORESTA NACIONAL DO PURUS': 'RESERVA EXTRATIVISTA DO M√âDIO PUR√öS',\n",
    "                    'FLORESTA NACIONAL DO TAPAJ√ìS': 'FLORESTA NACIONAL DE TAPAJ√ìS',\n",
    "                    'FLORESTA NACIONAL DO TAPIRAP√âAQUIRI': 'FLORESTA NACIONAL DE TAPIRAP√âAQUIRI',\n",
    "                    'FLORESTA NACIONAL MAPI√Å  INAUINI': 'FLORESTA NACIONAL DE MAPI√ÅINAUIN√ç',\n",
    "                    'PARQUE NACIONAL SERRA DA CUTIA': 'PARQUE NACIONAL DA SERRA DA CUTIA',\n",
    "                    'RESERVA BIOL√ìGICA NASCENTES DA SERRA DO CACHIMBO': 'RESERVA BIOL√ìGICA NASCENTES SERRA DO CACHIMBO',\n",
    "                    'RESERVA EXTRATIVISTA DO ALTO JURU√Å': 'RESERVA EXTRATIVISTA ALTO JURU√Å',\n",
    "                    'RESERVA EXTRATIVISTA DO ALTO TARAUAC√Å': 'RESERVA EXTRATIVISTA ALTO TARAUAC√Å',\n",
    "                    'RESERVA EXTRATIVISTA DO BAIXO JURU√Å': 'RESERVA EXTRATIVISTA BAIXO JURU√Å',\n",
    "                    'RESERVA EXTRATIVISTA DO CIRIACO': 'RESERVA EXTRATIVISTA DO CIRI√ÅCO',\n",
    "                    'RESERVA EXTRATIVISTA DO LAGO DO CUNI√É': 'RESERVA EXTRATIVISTA LAGO DO CUNI√É',\n",
    "                    'RESERVA EXTRATIVISTA DO M√âDIO JURU√Å': 'RESERVA EXTRATIVISTA M√âDIO JURU√Å',\n",
    "                    'RESERVA EXTRATIVISTA DO RIO CAJARI': 'RESERVA EXTRATIVISTA RIO CAJARI',\n",
    "                    'RESERVA EXTRATIVISTA DO RIO DO CAUT√ÅRIO': 'RESERVA EXTRATIVISTA RIO CAUT√ÅRIO',\n",
    "                    'RESERVA EXTRATIVISTA DO RIO OURO PRETO': 'RESERVA EXTRATIVISTA RIO OURO PRETO',\n",
    "                    'RESERVA EXTRATIVISTA RIO UNINI': 'RESERVA EXTRATIVISTA DO RIO UNINI',\n",
    "                    'RESERVA EXTRATIVISTA TAPAJ√ìSARAPIUNS': 'RESERVA EXTRATIVISTA TAPAJ√ìS ARAPIUNS',\n",
    "                    'RESERVA EXTRATIVISTA TAPAJ√ìS-ARAPIUNS': 'RESERVA EXTRATIVISTA TAPAJ√ìS ARAPIUNS',\n",
    "                    'RESERVA EXTRATIVISTA TERRA GRANDE  PRACU√öBA': 'RESERVA EXTRATIVISTA TERRA GRANDE PRACUUBA',\n",
    "                    'RESERVA EXTRATIVISTA TERRA GRANDE - PRACU√öBA': 'RESERVA EXTRATIVISTA TERRA GRANDE PRACUUBA',\n",
    "                    '√ÅREA DE PROTE√á√ÉO AMBIENTAL DOS MEANDROS DO RIO ARAGUAIA': '√ÅREA DE PROTE√á√ÉO AMBIENTAL MEANDROS DO ARAGUAIA',\n",
    "                    '√ÅREA DE RELEVANTE INTERESSE ECOL√ìGICO SERINGAL NOVA ESPERAN√áA': '√ÅREA DE RELEVANTE INTERESSE ECOL√ìGICA SERINGAL NOVA ESPERAN√áA',\n",
    "                    'ESTA√á√ÉO ECOL√ìGICA JUAMI-JAPUR√Å': 'ESTA√á√ÉO ECOL√ìGICA JUAMIJAPUR√Å',\n",
    "                    'FLORESTA NACIONAL DE BALATA-TUFARI': 'FLORESTA NACIONAL DE BALATATUFARI',\n",
    "                    'FLORESTA NACIONAL DE SARAC√Å-TAQUERA': 'FLORESTA NACIONAL DE SARAC√ÅTAQUERA',\n",
    "                    'FLORESTA NACIONAL MAPI√Å - INAUINI': 'FLORESTA NACIONAL DE MAPI√ÅINAUIN√ç',\n",
    "                    'RESERVA EXTRATIVISTA AUAT√ç-PARAN√Å': 'RESERVA EXTRATIVISTA AUAT√çPARAN√Å',\n",
    "                    'RESERVA EXTRATIVISTA DO CAZUMB√Å-IRACEMA': 'RESERVA EXTRATIVISTA DO CAZUMB√ÅIRACEMA',\n",
    "                    'RESERVA EXTRATIVISTA GURUP√Å-MELGA√áO': 'RESERVA EXTRATIVISTA GURUP√ÅMELGA√áO',\n",
    "                    'RESERVA EXTRATIVISTA IPA√ö-ANILZINHO': 'RESERVA EXTRATIVISTA IPA√öANILZINHO'}\n",
    "    alerts_uc['UC'] = alerts_uc['UC'].replace(dic_correcao)\n",
    "    gc_uc = alerts_uc.groupby('UC')['AREAMUNKM'].sum().reset_index()\n",
    "    gc_uc['UC'] = gc_uc['UC'].replace(dic_correcao)\n",
    "    \n",
    "\n",
    "    def uc_geodf(state):\n",
    "        first_alert = alerts_df[alerts_df['UC'] == state]['geometry'].iloc[0]\n",
    "        representative_point = first_alert\n",
    "        new_record = {\n",
    "            'UC': state,\n",
    "            'geometry': representative_point\n",
    "        }\n",
    "        return gpd.GeoDataFrame([new_record], crs=c_units_copy.crs)\n",
    "    \n",
    "    c_units_copy = pd.concat([c_units_copy, uc_geodf('ESTA√á√ÉO ECOL√ìGICA DE CARACARA√ç')], ignore_index=True)\n",
    "    c_units_copy = pd.concat([c_units_copy, uc_geodf('ESTA√á√ÉO ECOL√ìGICA DE IQU√ä')], ignore_index=True)\n",
    "    \n",
    "    merge_ucs = pd.merge(c_units_copy, gc_uc, on='UC', how='left').fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    #############       Folium       #############\n",
    "\n",
    "    map = folium_map_init()\n",
    "\n",
    "    style_legal_amazon = {'fillOpacity':0 ,'color' : '#117306', 'weight': 3}\n",
    "    folium.GeoJson(legal_amazon, name = 'Legal Amazon', style_function= lambda x: style_legal_amazon).add_to(map)\n",
    "\n",
    "    style_states = {'fillOpacity':0 ,'color' : '#117306', 'weight': 2}\n",
    "    folium.GeoJson(states, name = 'States', style_function= lambda x: style_states).add_to(map)\n",
    "\n",
    "    style_ucs = {'fillOpacity':0 ,'color' : '#3d1601', 'weight': 1}\n",
    "    folium.GeoJson(c_units_copy, name = 'Conservation Units', style_function= lambda x: style_ucs).add_to(map)\n",
    "\n",
    "\n",
    "    folium.Choropleth(geo_data=merge_ucs.to_json(),\n",
    "                  name='Choropleth',\n",
    "                  data=merge_ucs,\n",
    "                  columns=['UC', 'AREAMUNKM'],\n",
    "                  key_on = 'feature.properties.UC',\n",
    "                  fill_color = 'YlOrRd',\n",
    "                  nan_fill_color = 'white',\n",
    "                  highlight = True,\n",
    "                  legend_name='Affected Area in km¬≤').add_to(map)\n",
    "    \n",
    "    marker_cluster = MarkerCluster().add_to(map)\n",
    "\n",
    "    marker_cluster = folium_add_markers(marker_cluster,merge_ucs,merge_ucs, 2, alerts_uc, 'UC', 'UC', texts['total_dmg'], 'AREAMUNKM')\n",
    "\n",
    "    folium.LayerControl().add_to(map)\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c52c5d61-c920-48b2-a9e2-f2e862182f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes, texts = get_texts()\n",
    "map = c_units_map()\n",
    "save_map('C_Units_EN',map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
